{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "633016c9",
   "metadata": {},
   "source": [
    "## Iterative Methods 2\n",
    "\n",
    "**Task 1.** Implement Minimal residual method and gradient descent using $\\texttt{numpy}$ library. Compare these two methods and conjugate gradient method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e42b4",
   "metadata": {},
   "source": [
    "## Conjugate gradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a585b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matrix\n",
    "A = np.array([\n",
    "    [-4,1,0,1,0,0,0,0,0],\n",
    "    [1,-4,1,0,1,0,0,0,0],\n",
    "    [0,1,-4,0,0,1,0,0,0],\n",
    "    [1,0,0,-4,1,0,1,0,0],\n",
    "    [0,1,0,1,-4,1,0,1,0],\n",
    "    [0,0,1,0,1,-4,0,0,1],\n",
    "    [0,0,0,1,0,0,-4,1,0],\n",
    "    [0,0,0,0,1,0,1,-4,1],\n",
    "    [0,0,0,0,0,1,0,1,-4]])\n",
    "\n",
    "b = np.array([-1,5,1,-1,-2,7,0,2,1]) # Column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7769a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_conjugate_gradient(A,b,steps=50,accuracy=10**(-8)):\n",
    "    \"\"\"\n",
    "    Function that solves linear equation Ax=b using conjugate gradient method.\n",
    "    \n",
    "    Input:\n",
    "    A - matrix\n",
    "    b - column vector\n",
    "    steps - (optional, 50 by default) max number of steps to make\n",
    "    accuracy - (optional, 10**(-8) by default) desired accuracy\n",
    "    \n",
    "    Output:\n",
    "    x - solution\n",
    "    k - number of steps taken to reach the solution\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(A) # Getting size of a mtrix\n",
    "    p = r = b # Select start p and r\n",
    "    x = np.zeros((n,)) # Picking a random x\n",
    "    rr = b.dot(b)\n",
    "\n",
    "    for k in range(steps):\n",
    "        Ap = A.dot(p)\n",
    "        tau = rr/p.dot(Ap)\n",
    "        x = x + tau*p\n",
    "        r = r - tau*Ap\n",
    "        rr_new = r.dot(r)\n",
    "        alpha = rr_new/rr\n",
    "        p = r + alpha*p\n",
    "        rr = rr_new\n",
    "        if np.linalg.norm(r) < accuracy:\n",
    "            return x, k\n",
    "\n",
    "    return x, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e454bb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution is [-0.25446429 -1.89285714 -1.37946429 -0.125      -0.9375     -2.625\n",
      " -0.30803571 -1.10714286 -1.18303571]\n",
      "Number of steps taken 4\n"
     ]
    }
   ],
   "source": [
    "x, steps = solve_conjugate_gradient(A, b)\n",
    "print(\"Solution is\", x)\n",
    "print(\"Number of steps taken\", steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af86db",
   "metadata": {},
   "source": [
    "## Minimal residual method\n",
    "\n",
    "Iteration step:\n",
    "$$\n",
    "\\boldsymbol{x}^{[k+1]}=\\boldsymbol{x}^{[k]} + \\tau_k(\\boldsymbol{b}-\\mathbf{A}\\boldsymbol{x}^{[k]})\n",
    "$$\n",
    "\n",
    "In minimal residual method we choose:\n",
    "$$\n",
    "\\tau_k := \\frac{\\langle \\mathbf{A}\\boldsymbol{r}^{[k]},\\boldsymbol{r}^{[k]}\\rangle}{\\langle \\mathbf{A}\\boldsymbol{r}^{[k]},\\mathbf{A}\\boldsymbol{r}^{[k]}\\rangle}, \\; \\boldsymbol{r}^{[k]}=\\boldsymbol{b}-\\mathbf{A}\\boldsymbol{x}^{[k]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b085211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_minimal_residual(A,b,steps=100,accuracy=10**(-8)):\n",
    "    \"\"\"\n",
    "    Function that solves linear equation Ax=b using minimal residual method.\n",
    "    \n",
    "    Input:\n",
    "    A - matrix\n",
    "    b - column vector\n",
    "    steps - (optional, 50 by default) max number of steps to make\n",
    "    accuracy - (optional, 10**(-8) by default) desired accuracy\n",
    "    \n",
    "    Output:\n",
    "    x - solution\n",
    "    k - number of steps taken to reach the solution\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(A) # Getting size of a mtrix\n",
    "    x = np.zeros((n,)) # Initializing a random initial vector x\n",
    "    r = b # Since x = 0, we have r = b - A*0 = b\n",
    "    for k in range(steps):\n",
    "        Ar = A.dot(r) # Calculating Ar^k\n",
    "        tau = (Ar.dot(r))/(Ar.dot(Ar)) # Calculating tau_k\n",
    "        x = x + tau*r # Updating x\n",
    "        r = b - A.dot(x) # Updating r\n",
    "        if np.linalg.norm(r) < accuracy:\n",
    "            return x, k\n",
    "    return x, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c40ece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution is [-0.25446428 -1.89285714 -1.37946428 -0.125      -0.9375     -2.625\n",
      " -0.30803571 -1.10714286 -1.18303571]\n",
      "Number of steps taken 55\n"
     ]
    }
   ],
   "source": [
    "x, steps = solve_minimal_residual(A, b)\n",
    "print(\"Solution is\", x)\n",
    "print(\"Number of steps taken\", steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9a325",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Essentially the same as _Minimal residual method_, but we choose $\\tau_k$ as follows:\n",
    "$$\n",
    "\\tau_k = \\frac{\\langle \\boldsymbol{r}^{[k]},\\boldsymbol{r}^{[k]} \\rangle}{\\langle \\mathbf{A}\\boldsymbol{r}^{[k]},\\boldsymbol{r}^{[k]} \\rangle}, \\; \\boldsymbol{r}^{[k]} = \\boldsymbol{b}-\\mathbf{A}\\boldsymbol{x}^{[k]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1beff423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_gradient_descent(A,b,steps=100,accuracy=10**(-8)):\n",
    "    \"\"\"\n",
    "    Function that solves linear equation Ax=b using gradient descent method.\n",
    "    \n",
    "    Input:\n",
    "    A - matrix\n",
    "    b - column vector\n",
    "    steps - (optional, 50 by default) max number of steps to make\n",
    "    accuracy - (optional, 10**(-8) by default) desired accuracy\n",
    "    \n",
    "    Output:\n",
    "    x - solution\n",
    "    k - number of steps taken to reach the solution\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(A) # Getting size of a mtrix\n",
    "    x = np.zeros((n,)) # Initializing a random initial vector x\n",
    "    r = b # Since x = 0, we have r = b - A*0 = b\n",
    "    for k in range(steps):\n",
    "        Ar = A.dot(r) # Calculating Ar^k\n",
    "        tau = (r.dot(r))/(Ar.dot(r)) # Calculating tau_k\n",
    "        x = x + tau*r # Updating x\n",
    "        r = b - A.dot(x) # Finding residual b - Ax^k\n",
    "        if np.linalg.norm(r) < accuracy:\n",
    "            return x, k\n",
    "    return x, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a7425d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution is [-0.25446428 -1.89285714 -1.37946428 -0.125      -0.9375     -2.625\n",
      " -0.30803571 -1.10714286 -1.18303571]\n",
      "Number of steps taken 58\n"
     ]
    }
   ],
   "source": [
    "x, steps = solve_gradient_descent(A, b)\n",
    "print(\"Solution is\", x)\n",
    "print(\"Number of steps taken\", steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab13327",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "Conjugate gradient descent is the fastest among the triplet, while gradient descent and minimal residual methods gave almost the same result for our given matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443ada4",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Given matrix\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "\\boldsymbol{T} & \\boldsymbol{E} & & & & \\\\\n",
    "\\boldsymbol{E} & \\boldsymbol{T} & \\boldsymbol{E} & & & \\\\\n",
    "& \\boldsymbol{E} & \\boldsymbol{T} & \\boldsymbol{E} & \\\\\n",
    "& & \\boldsymbol{E} & \\boldsymbol{T} & \\boldsymbol{E} & & \\\\\n",
    "& & & \\ddots & \\ddots & \\ddots & \\\\\n",
    "& & & & & & \\boldsymbol{E} & \\boldsymbol{T}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "figure out a way to avoid storing it as a whole when peforming computational methods. In other words, find a formula for $\\boldsymbol{A}\\boldsymbol{p}$.\n",
    "\n",
    "**Solution**. Consider a simpler case. Suppose $\\mathbf{T} = \\begin{bmatrix} t_{11} & t_{12} \\\\ t_{21} & t_{22} \\end{bmatrix}$ and we have $\\mathbf{A} = \\begin{bmatrix} \\mathbf{T} & \\mathbf{E} \\\\ \\mathbf{E} & \\mathbf{T} \\end{bmatrix}$, or, in other words:\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "t_{11} & t_{12} & 1 & 0 \\\\\n",
    "t_{21} & t_{22} & 0 & 1 \\\\\n",
    "1 & 0 & t_{11} & t_{12} \\\\\n",
    "0 & 1 & t_{21} & t_{22}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we multiply this matrix by a vector we obtain:\n",
    "$$\n",
    "\\mathbf{A}\\boldsymbol{p} = \\begin{bmatrix}\n",
    "t_{11} & t_{12} & 1 & 0 \\\\\n",
    "t_{21} & t_{22} & 0 & 1 \\\\\n",
    "1 & 0 & t_{11} & t_{12} \\\\\n",
    "0 & 1 & t_{21} & t_{22}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "p_1 \\\\ p_2 \\\\ p_3 \\\\ p_4\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "p_1t_{11} + p_2t_{12} + p_3 \\\\\n",
    "p_1t_{21} + p_2t_{22}+p_4 \\\\\n",
    "p_1 + p_3t_{11}+p_4t_{12}\\\\\n",
    "p_2 + p_3t_{21}+p_4t_{22}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Notice, that if we denote $\\boldsymbol{p}_{[1:2]}=\\begin{bmatrix}p_1 \\\\ p_2\\end{bmatrix}$ and $\\boldsymbol{p}_{[3:4]}=\\begin{bmatrix}p_3 \\\\ p_4\\end{bmatrix}$, our resultant vector is essentially a vector whose coordinates are (aligned in a column):\n",
    "$$\n",
    "\\mathbf{T}\\boldsymbol{p}_{[1:2]}+\\boldsymbol{p}_{[3:4]}, \\; \\mathbf{T}\\boldsymbol{p}_{[3:4]}+\\boldsymbol{p}_{[1:2]} \n",
    "$$\n",
    "\n",
    "Let us now generalize this idea.\n",
    "\n",
    "**Generalization** Suppose we have a vector:\n",
    "$$\n",
    "\\boldsymbol{p} = \\begin{bmatrix}\\boldsymbol{p}_{1} \\\\ \\boldsymbol{p}_{2} \\\\ \\vdots \\\\ \\boldsymbol{p}_{n-1}\\end{bmatrix} \\in \\mathbb{R}^{(n-1)^2}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{p}_j$ is a column vector of size $n-1$\n",
    "\n",
    "In that case,\n",
    "$$\n",
    "\\mathbf{A}\\boldsymbol{p} = \\begin{bmatrix}\n",
    "\\mathbf{T}\\boldsymbol{p}_1 + \\boldsymbol{p}_2 \\\\\n",
    "\\boldsymbol{p}_1 + \\mathbf{T}\\boldsymbol{p}_2 + \\boldsymbol{p}_3 \\\\\n",
    "\\boldsymbol{p}_2 + \\mathbf{T}\\boldsymbol{p}_3 + \\boldsymbol{p}_4 \\\\\n",
    "\\vdots\n",
    "\\\\\n",
    "\\mathbf{T}\\boldsymbol{p}_{n-2} + \\boldsymbol{p}_{n-1} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that $\\mathbf{T}=\\begin{bmatrix}\n",
    "-4 & 1 & 0 & 0 & 0 & \\dots & 0\\\\\n",
    "1 & -4 & 1 & 0 & 0 & \\dots & 0\\\\\n",
    "0 & 1 & -4 & 1 & 0 & \\dots & 0\\\\\n",
    "0 & 0 & 1 & -4 & 1 & \\dots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & 0 & 0 & \\dots & -4\n",
    "\\end{bmatrix}$ and likewise,\n",
    "$$\n",
    "\\mathbf{T}\\boldsymbol{p}_i = \\begin{bmatrix}\n",
    "-4p_{i,1}+p_{i,2} \\\\\n",
    "p_{i,1}-4p_{i,2}+p_{i,3} \\\\\n",
    "p_{i,2}-4p_{i,3}+p_{i,4} \\\\\n",
    "\\vdots\\\\\n",
    "p_{i,n-2}-4p_{i,n-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Therefore, complexity of finding $\\mathbf{T}\\boldsymbol{p}_i$ is $\\mathcal{O}(n)$, and thus complexity of finding $\\mathbf{A}\\boldsymbol{p}$ as a whole is $\\mathcal{O}(n^2)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newest",
   "language": "python",
   "name": "newest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
