@article{cv-survey,
  title={Computation-efficient deep learning for computer vision: A survey},
  author={Wang, Yulin and Han, Yizeng and Wang, Chaofei and Song, Shiji and Tian, Qi and Huang, Gao},
  journal={Cybernetics and Intelligence},
  year={2024},
  publisher={TUP}
}

@article{nlp-survey,
  title={Large language models meet nlp: A survey},
  author={Qin, Libo and Chen, Qiguang and Feng, Xiachong and Wu, Yang and Zhang, Yongheng and Li, Yinghui and Li, Min and Che, Wanxiang and Yu, Philip S},
  journal={arXiv preprint arXiv:2405.12819},
  year={2024}
}

@misc{kan-cnn,
      title={Convolutional Kolmogorov-Arnold Networks}, 
      author={Alexander Dylan Bodner and Antonio Santiago Tepsich and Jack Natan Spolski and Santiago Pourteau},
      year={2025},
      eprint={2406.13155},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.13155}, 
}

@article{b-splines,
    author = {Hasan, Shahid and Alam, Md. Nur and Fayz-Al-Asad, Md and Muhammad, Noor and Tunç, Cemil},
    year = {2024},
    month = {12},
    pages = {},
    title = {B-spline curve theory: An overview and applications in real life},
    volume = {13},
    journal = {Nonlinear Engineering},
    doi = {10.1515/nleng-2024-0054}
}

@article{recommendation-systems-survey,
  title={Recent developments in recommender systems: A survey},
  author={Li, Yang and Liu, Kangbo and Satapathy, Ranjan and Wang, Suhang and Cambria, Erik},
  journal={IEEE Computational Intelligence Magazine},
  volume={19},
  number={2},
  pages={78--95},
  year={2024},
  publisher={IEEE}
}

@article{biometrics-survey,
  title={Biometrics recognition using deep learning: A survey},
  author={Minaee, Shervin and Abdolrashidi, Amirali and Su, Hang and Bennamoun, Mohammed and Zhang, David},
  journal={Artificial Intelligence Review},
  volume={56},
  number={8},
  pages={8647--8695},
  year={2023},
  publisher={Springer}
}

@article{our-biometrics-1,
	author = {Dmytro Zakharov and Oleksandr Kuznetsov and Emanuele Frontoni},
	doi = {https://doi.org/10.1016/j.engappai.2024.109164},
	issn = {0952-1976},
	journal = {Engineering Applications of Artificial Intelligence},
	keywords = {Cancelable biometrics, Deep learning, Triplet loss, Feature extraction, Convolutional neural networks, Information security},
	pages = {109164},
	title = {Unrecognizable yet identifiable: Image distortion with preserved embeddings},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197624013228},
	volume = {137},
	year = {2024},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0952197624013228},
	bdsk-url-2 = {https://doi.org/10.1016/j.engappai.2024.109164}}

@article{our-biometrics-2,
	author = {Oleksandr Kuznetsov and Dmytro Zakharov and Emanuele Frontoni and Andrea Maranesi},
	doi = {https://doi.org/10.1016/j.cose.2024.103828},
	issn = {0167-4048},
	journal = {Computers \& Security},
	keywords = {Biometric authentication, Convolutional neural networks, Liveness detection, Spoofing attacks, Deep learning architectures, Security and robustness},
	pages = {103828},
	title = {AttackNet: Enhancing biometric security via tailored convolutional neural network architectures for liveness detection},
	url = {https://www.sciencedirect.com/science/article/pii/S0167404824001299},
	volume = {141},
	year = {2024},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167404824001299},
	bdsk-url-2 = {https://doi.org/10.1016/j.cose.2024.103828}}

@article{our-biometrics-3,
	author = {Kuznetsov, Oleksandr and Zakharov, Dmytro and Frontoni, Emanuele},
	date = {2024/06/01},
	doi = {10.1007/s11042-023-17714-7},
	id = {Kuznetsov2024},
	isbn = {1573-7721},
	journal = {Multimedia Tools and Applications},
	number = {19},
	pages = {56909--56938},
	title = {Deep learning-based biometric cryptographic key generation with post-quantum security},
	url = {https://doi.org/10.1007/s11042-023-17714-7},
	volume = {83},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s11042-023-17714-7}}

@article{chatgpt,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{gan,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{gan-survey,
  title={A survey on GANs for computer vision: Recent research, analysis and taxonomy},
  author={Iglesias, Guillermo and Talavera, Edgar and D{\'\i}az-{\'A}lvarez, Alberto},
  journal={Computer Science Review},
  volume={48},
  pages={100553},
  year={2023},
  publisher={Elsevier}
}

@article{mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@INPROCEEDINGS{yolo,
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={You Only Look Once: Unified, Real-Time Object Detection}, 
  year={2016},
  volume={},
  number={},
  pages={779-788},
  doi={10.1109/CVPR.2016.91}
}

@article{lecun,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995},
  publisher={Citeseer}
}

@article{attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}


@article{cybenko,
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	author = {Cybenko, G. },
	date = {1989/12/01},
	date-added = {2024-11-11 1:48:34 AM +0200},
	date-modified = {2024-11-11 1:48:34 AM +0200},
	doi = {10.1007/BF02551274},
	id = {Cybenko1989},
	isbn = {1435-568X},
	journal = {Mathematics of Control, Signals and Systems},
	number = {4},
	pages = {303--314},
	title = {Approximation by superpositions of a sigmoidal function},
	url = {https://doi.org/10.1007/BF02551274},
	volume = {2},
	year = {1989},
	bdsk-url-1 = {https://doi.org/10.1007/BF02551274}}

@article{old-nets,
  title={An introduction to computing with neural nets},
  author={Richard Lippmann},
  journal={IEEE ASSP Magazine},
  year={1987},
  volume={4},
  pages={4-22},
  url={https://api.semanticscholar.org/CorpusID:8275028}
}

@article{ka-explained,
	author = {Morris, Sidney},
	doi = {10.1090/bull/1698},
	journal = {Bulletin of the American Mathematical Society},
	month = {07},
	pages = {1},
	title = {Hilbert 13: Are there any genuine continuous multivariate real-valued functions?},
	volume = {58},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1090/bull/1698}}

@article{kan,
  title={Kan: Kolmogorov-arnold networks},
  author={Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v{c}}i{\'c}, Marin and Hou, Thomas Y and Tegmark, Max},
  journal={arXiv preprint arXiv:2404.19756},
  year={2024}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{kolmogorov-original,
author="Kolmogorov A. N.",
title="On the Representation of Continuous Functions of one Variable and Addition",
journal="Doklady Akademii Nauk SSSR",
publisher="American Mathematical Society",
year="1957",
volume="144",
pages="679-681",
URL="https://cir.nii.ac.jp/crid/1571980075616322176"
}

@article{scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{saturated-fns,
  title={Revise Saturated Activation Functions},
  author={Bing Xu and Ruitong Huang and Mu Li},
  journal={ArXiv},
  year={2016},
  volume={abs/1602.05980},
  url={https://api.semanticscholar.org/CorpusID:9627506}
}

@article{params-generalization,
    author = {Baum, Eric B. and Haussler, David},
    title = {What Size Net Gives Valid Generalization?},
    journal = {Neural Computation},
    volume = {1},
    number = {1},
    pages = {151-160},
    year = {1989},
    month = {03},
    abstract = {We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 \&lt; ∊ ≤ 1/8. We show that if m ≥ O(W/∊ log N/∊) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 − ∊/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 − ∊ of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than Ω(W/∊) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 − ∊ fraction of the future test examples.},
    issn = {0899-7667},
    doi = {10.1162/neco.1989.1.1.151},
    url = {https://doi.org/10.1162/neco.1989.1.1.151},
    eprint = {https://direct.mit.edu/neco/article-pdf/1/1/151/811817/neco.1989.1.1.151.pdf},
}

@article{params-generalization-2,
	author = {Barron, Andrew},
	doi = {10.1007/BF00993164},
	isbn = {9781558602137},
	journal = {Machine Learning},
	month = {01},
	pages = {115-133},
	title = {Approximation and Estimation Bounds for Artificial Neural Networks},
	volume = {14},
	year = {1994},
	bdsk-url-1 = {https://doi.org/10.1007/BF00993164}}

@book{book-1,
 author = "Kevin P. Murphy",
 title = "Probabilistic Machine Learning: An introduction",
 publisher = "MIT Press",
 year = 2022,
 url = "probml.ai"
}

@article{book-2,
  title={Dive into deep learning},
  author={Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  journal={arXiv preprint arXiv:2106.11342},
  year={2021}
}

@INPROCEEDINGS{eigenface,
  author={Turk, M.A. and Pentland, A.P.},
  booktitle={Proceedings. 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, 
  title={Face recognition using eigenfaces}, 
  year={1991},
  volume={},
  number={},
  pages={586-591},
  keywords={Face recognition;Face detection;Humans;Character recognition;Computer vision;Head;Eyes;Nose;Computational modeling;Image recognition},
  doi={10.1109/CVPR.1991.139758}}

@ARTICLE{fisherface,
  author={Belhumeur, P.N. and Hespanha, J.P. and Kriegman, D.J.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Eigenfaces vs. Fisherfaces: recognition using class specific linear projection}, 
  year={1997},
  volume={19},
  number={7},
  pages={711-720},
  keywords={Face recognition;Light scattering;Lighting;Face detection;Principal component analysis;Shadow mapping;Light sources;Pattern classification;Pixel;Error analysis},
  doi={10.1109/34.598228}}

@article{pca,
    title = {Principal components analysis (PCA)},
    journal = {Computers and Geosciences},
    volume = {19},
    number = {3},
    pages = {303-342},
    year = {1993},
    issn = {0098-3004},
    doi = {https://doi.org/10.1016/0098-3004(93)90090-R},
    url = {https://www.sciencedirect.com/science/article/pii/009830049390090R},
    author = {Andrzej Mackiewicz and Waldemar Ratajczak},
    keywords = {Principal Components Analysis, Variance-covariance matrix, Coefficients of determination, Eigenvalues, Eigenvectors, Correlation matrix, Bartlett's statistics, FORTRAN 77},
    abstract = {Principal Components Analysis (PCA) as a method of multivariate statistics was created before the Second World War. However, the wider application of this method only occurred in the 1960s, during the “Quantitative Revolution” in the Natural and Social Sciences. The main reason for this time-lag was the huge difficulty posed by calculations involving this method. Only with the advent and development of computers did the almost unlimited application of multivariate statistical methods, including principal components, become possible. At the same time, requirements arose for precise numerical methods concerning, among other things, the calculation of eigenvalues and eigenvectors, because the application of principal components to technical problems required absolute accuracy. On the other hand, numerous applications in Social Sciences gave rise to a significant increase in the ability to interpret these nonobservable variables, which is just what the principal components are. In the application of principal components, the problem is not only to do with their formal properties but above all, their empirical origins. The authors considered these two tendencies during the creation of the program for principal components. This program—entitled PCA—accompanies this paper. It analyzes consecutively, matrices of variance-covariance and correlations, and performs the following functions: •- the determination of eigenvalues and eigenvectors of these matrices.•- the testing of principal components.•- the calculation of coefficients of determination between selected components and the initial variables, and the testing of these coefficients,•- the determination of the share of variation of all the initial variables in the variation of particular components,•- construction of a dendrite for the initial set of variables,•- the construction of a dendrite for a selected pattern of the principal components,•- the scatter of the objects studied in a selected coordinate system. Thus, the PCA program performs many more functions especially in testing and graphics, than PCA programs in conventional statistical packages. Included in this paper are a theoretical description of principal components, the basic rules for their interpretation and also statistical testing.}
}
