@article{cv-survey,
  title={Computation-efficient deep learning for computer vision: A survey},
  author={Wang, Yulin and Han, Yizeng and Wang, Chaofei and Song, Shiji and Tian, Qi and Huang, Gao},
  journal={Cybernetics and Intelligence},
  year={2024},
  publisher={TUP}
}

@article{nlp-survey,
  title={Large language models meet nlp: A survey},
  author={Qin, Libo and Chen, Qiguang and Feng, Xiachong and Wu, Yang and Zhang, Yongheng and Li, Yinghui and Li, Min and Che, Wanxiang and Yu, Philip S},
  journal={arXiv preprint arXiv:2405.12819},
  year={2024}
}

@article{recommendation-systems-survey,
  title={Recent developments in recommender systems: A survey},
  author={Li, Yang and Liu, Kangbo and Satapathy, Ranjan and Wang, Suhang and Cambria, Erik},
  journal={IEEE Computational Intelligence Magazine},
  volume={19},
  number={2},
  pages={78--95},
  year={2024},
  publisher={IEEE}
}

@article{biometrics-survey,
  title={Biometrics recognition using deep learning: A survey},
  author={Minaee, Shervin and Abdolrashidi, Amirali and Su, Hang and Bennamoun, Mohammed and Zhang, David},
  journal={Artificial Intelligence Review},
  volume={56},
  number={8},
  pages={8647--8695},
  year={2023},
  publisher={Springer}
}

@article{our-biometrics-1,
	author = {Dmytro Zakharov and Oleksandr Kuznetsov and Emanuele Frontoni},
	doi = {https://doi.org/10.1016/j.engappai.2024.109164},
	issn = {0952-1976},
	journal = {Engineering Applications of Artificial Intelligence},
	keywords = {Cancelable biometrics, Deep learning, Triplet loss, Feature extraction, Convolutional neural networks, Information security},
	pages = {109164},
	title = {Unrecognizable yet identifiable: Image distortion with preserved embeddings},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197624013228},
	volume = {137},
	year = {2024},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0952197624013228},
	bdsk-url-2 = {https://doi.org/10.1016/j.engappai.2024.109164}}

@article{our-biometrics-2,
	author = {Oleksandr Kuznetsov and Dmytro Zakharov and Emanuele Frontoni and Andrea Maranesi},
	doi = {https://doi.org/10.1016/j.cose.2024.103828},
	issn = {0167-4048},
	journal = {Computers \& Security},
	keywords = {Biometric authentication, Convolutional neural networks, Liveness detection, Spoofing attacks, Deep learning architectures, Security and robustness},
	pages = {103828},
	title = {AttackNet: Enhancing biometric security via tailored convolutional neural network architectures for liveness detection},
	url = {https://www.sciencedirect.com/science/article/pii/S0167404824001299},
	volume = {141},
	year = {2024},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167404824001299},
	bdsk-url-2 = {https://doi.org/10.1016/j.cose.2024.103828}}

@article{our-biometrics-3,
	author = {Kuznetsov, Oleksandr and Zakharov, Dmytro and Frontoni, Emanuele},
	date = {2024/06/01},
	doi = {10.1007/s11042-023-17714-7},
	id = {Kuznetsov2024},
	isbn = {1573-7721},
	journal = {Multimedia Tools and Applications},
	number = {19},
	pages = {56909--56938},
	title = {Deep learning-based biometric cryptographic key generation with post-quantum security},
	url = {https://doi.org/10.1007/s11042-023-17714-7},
	volume = {83},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s11042-023-17714-7}}

@article{chatgpt,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{gan,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{gan-survey,
  title={A survey on GANs for computer vision: Recent research, analysis and taxonomy},
  author={Iglesias, Guillermo and Talavera, Edgar and D{\'\i}az-{\'A}lvarez, Alberto},
  journal={Computer Science Review},
  volume={48},
  pages={100553},
  year={2023},
  publisher={Elsevier}
}

@article{mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@INPROCEEDINGS{yolo,
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={You Only Look Once: Unified, Real-Time Object Detection}, 
  year={2016},
  volume={},
  number={},
  pages={779-788},
  doi={10.1109/CVPR.2016.91}
}

@article{lecun,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995},
  publisher={Citeseer}
}

@article{attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}


@article{cybenko,
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	author = {Cybenko, G. },
	date = {1989/12/01},
	date-added = {2024-11-11 1:48:34 AM +0200},
	date-modified = {2024-11-11 1:48:34 AM +0200},
	doi = {10.1007/BF02551274},
	id = {Cybenko1989},
	isbn = {1435-568X},
	journal = {Mathematics of Control, Signals and Systems},
	number = {4},
	pages = {303--314},
	title = {Approximation by superpositions of a sigmoidal function},
	url = {https://doi.org/10.1007/BF02551274},
	volume = {2},
	year = {1989},
	bdsk-url-1 = {https://doi.org/10.1007/BF02551274}}

@article{old-nets,
  title={An introduction to computing with neural nets},
  author={Richard Lippmann},
  journal={IEEE ASSP Magazine},
  year={1987},
  volume={4},
  pages={4-22},
  url={https://api.semanticscholar.org/CorpusID:8275028}
}

@article{ka-explained,
	author = {Morris, Sidney},
	doi = {10.1090/bull/1698},
	journal = {Bulletin of the American Mathematical Society},
	month = {07},
	pages = {1},
	title = {Hilbert 13: Are there any genuine continuous multivariate real-valued functions?},
	volume = {58},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1090/bull/1698}}

@article{kan,
  title={Kan: Kolmogorov-arnold networks},
  author={Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v{c}}i{\'c}, Marin and Hou, Thomas Y and Tegmark, Max},
  journal={arXiv preprint arXiv:2404.19756},
  year={2024}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{kolmogorov-original,
author="Kolmogorov A. N.",
title="On the Representation of Continuous Functions of one Variable and Addition",
journal="Doklady Akademii Nauk SSSR",
publisher="American Mathematical Society",
year="1957",
volume="144",
pages="679-681",
URL="https://cir.nii.ac.jp/crid/1571980075616322176"
}

@article{scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{saturated-fns,
  title={Revise Saturated Activation Functions},
  author={Bing Xu and Ruitong Huang and Mu Li},
  journal={ArXiv},
  year={2016},
  volume={abs/1602.05980},
  url={https://api.semanticscholar.org/CorpusID:9627506}
}

@article{params-generalization,
    author = {Baum, Eric B. and Haussler, David},
    title = {What Size Net Gives Valid Generalization?},
    journal = {Neural Computation},
    volume = {1},
    number = {1},
    pages = {151-160},
    year = {1989},
    month = {03},
    abstract = {We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 \&lt; ∊ ≤ 1/8. We show that if m ≥ O(W/∊ log N/∊) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 − ∊/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 − ∊ of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than Ω(W/∊) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 − ∊ fraction of the future test examples.},
    issn = {0899-7667},
    doi = {10.1162/neco.1989.1.1.151},
    url = {https://doi.org/10.1162/neco.1989.1.1.151},
    eprint = {https://direct.mit.edu/neco/article-pdf/1/1/151/811817/neco.1989.1.1.151.pdf},
}

@article{params-generalization-2,
	author = {Barron, Andrew},
	doi = {10.1007/BF00993164},
	isbn = {9781558602137},
	journal = {Machine Learning},
	month = {01},
	pages = {115-133},
	title = {Approximation and Estimation Bounds for Artificial Neural Networks},
	volume = {14},
	year = {1994},
	bdsk-url-1 = {https://doi.org/10.1007/BF00993164}}

@book{book-1,
 author = "Kevin P. Murphy",
 title = "Probabilistic Machine Learning: An introduction",
 publisher = "MIT Press",
 year = 2022,
 url = "probml.ai"
}

@article{book-2,
  title={Dive into deep learning},
  author={Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  journal={arXiv preprint arXiv:2106.11342},
  year={2021}
}
