\appendix

\chapter{Програмний код для тренування мережі Цибенко}\label{appendix:cybenko-code}

В цьому додатку, ми наведемо програмний код для тренування мережі Цибенко на
прикладі задачі класифікації. Для цього ми використаємо бібліотеку
\texttt{TensorFlow} на мові програмування \texttt{Python}. Для початку, ми 
імпортуємо необхідні бібліотеки:

\begin{lstlisting}[language=Python]
# Standard imports
from __future__ import annotations
from pathlib import Path

# Tensorflow and numpy imports
import tensorflow as tf
import numpy as np

# Matplotlib imports
import matplotlib.pyplot as plt
from matplotlib import ticker, cm 
from matplotlib.colors import LinearSegmentedColormap
\end{lstlisting}

Далі, створюємо функції для побудови графіків та генерації набору даних:

\begin{lstlisting}[language=Python]
# Selecting primary colors
PRIMARY_COLOR_POSITIVE = 'mediumspringgreen'
PRIMARY_COLOR_NEGATIVE = 'violet'
CMAP = LinearSegmentedColormap.from_list("Custom", [PRIMARY_COLOR_NEGATIVE, PRIMARY_COLOR_POSITIVE], N=20)

# Picking a number of points to draw
DATASET_SIZE = 1000
RADIUS = 0.8

def get_labels(x: np.ndarray) -> np.ndarray:
    """
    Based on array of R^2 coordinates, returns an array of bits indicating
    whether the point is included in the region
    """
    
    return 0.5*(x[:,1]-0.5)**2 - 0.4*(x[:,0]-0.5)**2 < 0.02

def generate_dataset() -> np.ndarray:
    """
    Generates a random dataset based on the curve provided
    """
    
    x = tf.random.uniform(shape=(DATASET_SIZE, 2))
    return x, get_labels(x)

def display_dataset(x: np.ndarray, y: np.ndarray, save_path: Path = None) -> None:
    """
    Displays the dataset in a form of a scatterplot
    
    Args:
        x - an array of points in R^2
        y - an array of bits, marking the class of each point
    """
    
    # Split the dataset into two parts
    x_positive = np.array([x for x, y in zip(x, y) if y])
    x_negative = np.array([x for x, y in zip(x, y) if not y])
    
    # Display two scatterplots
    fig, ax = plt.subplots()
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.0])
    plt.scatter(x_positive[:,0], x_positive[:,1], color=PRIMARY_COLOR_POSITIVE, edgecolors='black')
    plt.scatter(x_negative[:,0], x_negative[:,1], color=PRIMARY_COLOR_NEGATIVE, edgecolors='black')
    plt.grid()
    
    # Showing the plot and saving if needed
    if save_path is not None:
        plt.savefig(save_path, transparent=True)
    
    plt.show()

def display_dataset_with_heatmap(x: np.ndarray, 
                                    y: np.ndarray, 
                                    fn: callable, 
                                    save_path: Path = None) -> None:
    """
    Displays the dataset in a form of a scatterplot together
    with the heatmap plotted using fn function
    
    Args:
        x - an array of points in R^2
        y - an array of bits, marking the class of each point
        fn - function from R^2 to R, according to which the heatmap is built
        save_path - path where image is saved. Select None to omit saving
    """
    
    # Preparing the plot
    fig, ax = plt.subplots()

    # Show the heatmap
    x_nodes = np.linspace(0.0, 1.0, 3000)
    y_nodes = np.linspace(0.0, 1.0, 3000)
    xx, yy = np.meshgrid(x_nodes, y_nodes)
    r1, r2 = xx.flatten(), yy.flatten()
    r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))
    grid = np.hstack((r1, r2))
    prediction = np.array(fn(grid))
    zz = prediction.reshape(xx.shape)
    c = plt.contourf(xx, yy, zz, cmap=CMAP)
    fig.colorbar(c)

    # Split the dataset into two parts
    x_positive = np.array([x for x, y in zip(x, y) if y])
    x_negative = np.array([x for x, y in zip(x, y) if not y])
    
    # Display two scatterplots
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.0])
    plt.scatter(x_positive[:,0], x_positive[:,1], color=PRIMARY_COLOR_POSITIVE, edgecolors='black')
    plt.scatter(x_negative[:,0], x_negative[:,1], color=PRIMARY_COLOR_NEGATIVE, edgecolors='black')
    plt.grid()
    
    # Showing the plot and saving if needed
    if save_path is not None:
        plt.savefig(save_path, transparent=True)
    
    plt.show()
\end{lstlisting}

Генеруємо датасет та зберігаємо його візуалізацію:
\begin{lstlisting}[language=Python]
x, y = generate_dataset()
display_dataset(x, y, save_path='dataset.pdf')
display_dataset_with_heatmap(x, y, get_labels, save_path='./classification-example.pdf')
# Convert array of bits to array of 0.0's and 1.0's
y = tf.cast(y, tf.float32)
\end{lstlisting}

Далі, головна частина: клас для специфікації архітектури мережі Цибенко та її тренування:
\begin{lstlisting}[language=Python]
class CybenkoNetwork:
"""
Class representing the Cybenko network
"""

ACTIVATION = 'sigmoid'
INITIALIZER = 'GlorotNormal'
LEARNING_RATE = 0.05

def __init__(self, hidden_layer_size: int = 6, learning_rate: float = 0.05) -> None:
    """
    Initializes the CybenkoNetwork instance.
    
    Args:
        - hidden_layer_size: number of hidden neurons (n from paper)
        - learning_rate: how fast to train the network
    """
    
    self._hidden_layer = tf.keras.layers.Dense(HIDDEN_LAYER_SIZE, 
                        activation=CybenkoNetwork.ACTIVATION, 
                        bias_initializer=CybenkoNetwork.INITIALIZER,
                        kernel_initializer=CybenkoNetwork.INITIALIZER)
    self._alpha = tf.Variable(tf.random.normal((1, hidden_layer_size)), name='alpha')
    self._optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)
    self._mean = 0.5 # Value needed for final classification

def predict(self, x: np.ndarray) -> np.ndarray:
    """
    Based on the batch of inputs, gives a batch of predictions
    
    Args:
        x - batch of inputs
    """
    
    z = self._hidden_layer(x)
    return tf.matmul(self._alpha, tf.transpose(z))

def predict_binary(self, x: np.ndarray) -> np.ndarray:
    """
    Predicts the class of each x value in a form of bit
    
    Args:
        x - batch of inputs
    """
    
    prediction = self.predict(x)
    return prediction > self._mean

def train(self, x: np.ndarray, y: np.ndarray, epochs: int = 5000, batch_size: int = 1024) -> None:
    """
    Trains the model on given dataset (x, y) with the specified number of epochs 
    and batch size.
    
    Args:
        x, y - array of R^2 coordinates and corresponding label
        epochs - number of epochs to train with
        batch_size - number of pairs for each gradient iteration step
    """
    
    for epoch in range(epochs):
        for offset in range(0, len(x), batch_size):
            # Getting the batch
            xs, ys = x[offset: offset + batch_size], y[offset: offset + batch_size]

            with tf.GradientTape() as tape:
                # Forward pass: calculating the MSE loss
                loss_value = tf.reduce_mean((self.predict(xs) - np.array([ys]))**2)

            # Use the gradient tape to automatically retrieve
            # the gradients of the trainable variables with respect to the loss.
            grads = tape.gradient(loss_value, [self._alpha, *self._hidden_layer.trainable_variables])
            # Run one step of gradient descent by updating
            # the value of the variables to minimize the loss.
            self._optimizer.apply_gradients(zip(grads, [self._alpha, *self._hidden_layer.trainable_variables]))
        
        if (epoch + 1) % 100 == 0:
            print(f'Finished epoch {epoch+1}, loss value: {loss_value}...')
    
    print('Training finished!')
    # Calculating the mean score for the whole dataset 
    # (needed further to predict the class in the binary form)
    self._mean = np.mean(self.predict(x))
\end{lstlisting}

Далі, починаємо тренування:
\begin{lstlisting}[language=Python]
# Initializing and training the model
HIDDEN_LAYER_SIZE = 6
model = CybenkoNetwork(hidden_layer_size=HIDDEN_LAYER_SIZE, learning_rate=0.05)
model.train(x, y, epochs=5000, batch_size=1024)
\end{lstlisting}

Далі, залишається лише відобразити неперервне передбачення мережі,
дискретне передбачення та передбачення проміжних значень:
\begin{lstlisting}[language=Python]
display_dataset_with_heatmap(x, y, model.predict, save_path='classification-cont-prediction.pdf')
display_dataset_with_heatmap(x, y, model.predict_binary, save_path='classification-discr-prediction.pdf')
for i in range(HIDDEN_LAYER_SIZE):
    display_dataset_with_heatmap(x, y, 
        lambda x: np.array([model._hidden_layer(x)[:,i]]), 
        save_path=f'layer-{i+1}-prediction.pdf')
\end{lstlisting}

Нарешті, виводимо параметри моделі:
\begin{lstlisting}[language=Python]
print('Weights and biases:', model._hidden_layer.trainable_variables)
print('alpha weights:', model._alpha)
\end{lstlisting}

\chapter{Програмний код модуля $B$-сплайнів}\label{appendix:b-spline-code}
В цьому додатку, ми наведемо програмний код для реалізації модуля
$B$-сплайнів на фреймворку \texttt{PyTorch} на мові програмування
\texttt{Python}. 
\begin{lstlisting}[language=Python]
from __future__ import annotations

import torch
import torch.nn as nn

import numpy as np

class BSpline(nn.Module):
    """
    Class for B-spline interpolation using PyTorch.
    This class allows for the creation of B-spline curves with a specified number of control points and degree.
    The B-spline curve is defined by a set of control points and a knot vector.
    The B-spline basis functions are computed using the Cox-de Boor recursion formula.
    """
    
    def __init__(
        self, 
        knots: int, 
        degree: int = 3
    ) -> None:
        """
        Initializes the B-spline object.
        Args:
            knots (int): Number of control points.
            degree (int): Degree of the B-spline. Default is 3 (cubic B-spline).
        """
        
        super(BSpline, self).__init__()
        
        self.knots = knots
        self.degree = degree
        
        # Setting up the grid
        grid = torch.linspace(-1, 1, knots) # Knot vector of length knots + degree + 1
        grid = BSpline.extend_grid(grid, k=degree) # Extend the grid on either side by degree steps
        self.register_buffer('grid', grid)
        
        # Prepare trainable parameters
        coeffs_num = knots + 2 * degree - 1
        self.coeffs_num = coeffs_num
        self.coeffs = nn.Parameter(torch.randn(coeffs_num)) # Learnable coefficients
        

    @staticmethod
    def extend_grid(grid: np.ndarray, k: int) -> np.ndarray:
        """
        Extends the grid on either size by k steps

        Args:
            grid: number of splines x number of control points
            k: spline order

        Returns:
            new_grid: number of splines x (number of control points + 2 * k)
        """
        
        n_intervals = len(grid) - 1
        bucket_size = (grid[-1] - grid[0]) / n_intervals
        
        for i in range(k):
            grid = torch.cat([grid[:1] - bucket_size, grid])
            grid = torch.cat([grid, grid[-1:] + bucket_size])

        return grid
    

    def b_spline_basis(self, x: torch.Tensor) -> torch.Tensor:
        """
        Evaluates the B-spline basis functions at position x.
        """
        
        tensor_shape = x.shape
        basis = torch.zeros((self.knots+2*self.degree-1, self.degree+1, *tensor_shape), dtype=x.dtype, device=x.device)
        
        for i in range(self.knots+2*self.degree-1):
            basis[i, 0, :] = torch.where(
                (x >= self.grid[i]) & (x < self.grid[i+1]), 
                torch.ones_like(x), 
                torch.zeros_like(x)
            )
        
        for d in range(1, self.degree+1):
            for i in range(self.knots+2*self.degree-1-d):
                b1 = (x - self.grid[i]) * basis[i, d-1] / (self.grid[i+d] - self.grid[i])
                b2 = (self.grid[i+d+1] - x) * basis[i+1, d-1] / (self.grid[i+d+1] - self.grid[i+1])
                basis[i, d, :] = b1 + b2
        
        return basis[:, self.degree, ...] # The last column corresponds to the degree of the spline


    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Computes the forward pass of the B-spline.
        """
        
        basis_values = self.b_spline_basis(x)
        return basis_values.T @ self.coeffs
\end{lstlisting}

\chapter{Програмний код конволюційного шару Колмогорова-Арнольда}\label{appendix:c-ckan-code}

В цьому додатку, ми наведемо програмний код для реалізації конволюційного шару
Колмогорова-Арнольда на фреймворку \texttt{PyTorch} на мові програмування
\texttt{Python}.
\begin{lstlisting}[language=Python]
"""
Implementation of the KANConv2d layer as described in my
Bachelor's thesis. This layer is a convolutional layer that
uses a spline-based approach to learn the convolutional
kernel weights. The layer is designed to work with 2D input
data, such as images.
"""
from __future__ import annotations

from typing import Tuple, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class KANConv2d(nn.Module):
    """
    Implementation of the KANConv2d layer as described in my 
    Bachelor's thesis. This layer is a convolutional layer that
    uses a spline-based approach to learn the convolutional
    kernel weights. The layer is designed to work with 2D input
    data, such as images.
    """
    
    def __init__(
        self,
        in_channels: int, 
        out_channels: int, 
        kernel_size: Union[Tuple[int, int], int], 
        stride: int = 1,
        padding: int = 1,
        spline_points: int = 5, 
        spline_degree: int = 3,
        spline_coefficients_variance: float = 0.1,
    ) -> None:
        """
        - `in_channels`: Number of input channels
        - `out_channels`: Number of output channels
        - `kernel_size`: Size of the convolutional kernel (assumes to be square if of type `int`)
        - `stride`: Stride of the convolution
        - `padding`: Padding added to all sides of the input
        - `spline_points`: Number of points for the spline basis functions
        - `spline_coefficients_variance`: Variance of the spline coefficients initialization
        - `spline_degree`: Degree of the spline basis functions
        """
        
        super(KANConv2d, self).__init__()
        
        if isinstance(kernel_size, int):
            kernel_size = (kernel_size, kernel_size)
        if isinstance(stride, int):
            stride = (stride, stride)
        if isinstance(padding, int):
            padding = (padding, padding)
        
        # Set the internal parameters
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.knots = spline_points
        self.degree = spline_degree
        
        # Learnable weights for beta(x) and spine basis functions
        self.beta_weights = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size[0], kernel_size[1]))
        self.spline_weights = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size[0], kernel_size[1]))

        # Setting up the grid
        grid = torch.linspace(-1, 1, self.knots) # Knot vector of length knots + degree + 1
        grid = KANConv2d.extend_grid(grid, d=self.degree) # Extend the grid on either side by degree steps
        self.register_buffer('grid', grid)
        
        # Prepare trainable parameters
        spline_coefficients = self.knots + 2 * self.degree - 1 # Number of coefficients for the spline representation
        self.spline_coefficients = spline_coefficients
        # Initialize the spline coefficients according to the normal distribution
        # with the specified variance
        std = np.sqrt(spline_coefficients_variance)
        self.coeffs = nn.Parameter(std * torch.randn(spline_coefficients, out_channels, in_channels, kernel_size[0], kernel_size[1])) # Learnable coefficients


    @staticmethod
    def extend_grid(grid: np.ndarray, d: int) -> np.ndarray:
        """
        Extends the grid on either size by d steps

        Args:
            grid: number of splines x number of control points
            d: spline order

        Returns:
            new_grid: number of splines x (number of control points + 2*d)
        """
        
        n_intervals = len(grid) - 1
        bucket_size = (grid[-1] - grid[0]) / n_intervals
        
        for _ in range(d):
            grid = torch.cat([grid[:1] - bucket_size, grid])
            grid = torch.cat([grid, grid[-1:] + bucket_size])

        return grid

    
    def b_spline_basis(self, x: torch.Tensor) -> torch.Tensor:
        """
        Evaluates the B-spline basis functions at position x.
        """
        
        tensor_shape = x.shape

        # Using x.device for basis, assuming grid will be used accordingly or is on the same device
        basis = torch.zeros((self.knots+2*self.degree-1, self.degree+1, *tensor_shape), dtype=x.dtype, device=x.device)
        
        # Ensure grid is on the same device as x for operations involving both
        grid_local = self.grid.to(x.device)

        for i in range(self.spline_coefficients):
            # Ensure operations are between tensors on the same device
            condition = (x >= grid_local[i]) & (x < grid_local[i+1])
            basis[i, 0, ...] = torch.where(
                condition,
                torch.ones_like(x), 
                torch.zeros_like(x)
            )
        
        for d in range(1, self.degree+1):
            for i in range(self.spline_coefficients-d):
                # Clone the slices of `basis` from the previous degree (d-1) before using them.
                # This prevents the inplace modification of `basis` (via .copy_ below)
                # from affecting the versions of these tensors needed by autograd.
                basis_i_prev_d = basis[i, d-1, ...].clone()
                basis_i_plus_1_prev_d = basis[i+1, d-1, ...].clone()

                # Denominators
                den1 = grid_local[i+d] - grid_local[i]
                den2 = grid_local[i+d+1] - grid_local[i+1]

                # Calculate b1
                # Note: Original code divides directly. If den1 can be zero, this can lead to inf/nan.
                # This fix focuses on the autograd error, not numerical stability of division by zero.
                if den1 == 0: # Avoid division by zero; result of this term is effectively 0 if numerator is finite.
                              # Or handle as per specific B-spline convention for coincident knots.
                              # For now, if den1 is 0, b1 will be 0 assuming basis_i_prev_d is not inf/nan.
                    b1 = torch.zeros_like(x)
                else:
                    b1 = (x - grid_local[i]) * basis_i_prev_d / den1
                
                # Calculate b2
                if den2 == 0: # Similar handling for den2
                    b2 = torch.zeros_like(x)
                else:
                    b2 = (grid_local[i+d+1] - x) * basis_i_plus_1_prev_d / den2
                
                # The inplace copy operation was the source of the autograd error.
                # By using cloned inputs for b1 and b2, the original `basis` tensor's
                # versions are not an issue for *their* gradient computation.
                basis[i, d, ...].copy_(b1 + b2)
        
        return basis[:, self.degree, ...]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Computes the forward pass of the KANConv2d layer.
        """
        
        # First, figure out the input/output dimensions
        batch_size, _, height, width = x.shape
        x = F.pad(x, 
                (self.padding[1], 
                self.padding[1], 
                self.padding[0], 
                self.padding[0]))
        out_height = (height + 2 * self.padding[0] - self.kernel_size[0]) // self.stride[0] + 1
        out_width = (width + 2 * self.padding[1] - self.kernel_size[1]) // self.stride[1] + 1
        output = torch.zeros(
            batch_size, 
            self.out_channels, 
            out_height, 
            out_width, 
            device=x.device
        )
        
        for i in range(out_height):
            for j in range(out_width):
                patch = x[:, :, 
                        i*self.stride[0]:i*self.stride[0]+self.kernel_size[0],
                        j*self.stride[1]:j*self.stride[1]+self.kernel_size[1]]
                
                # Evaluate the beta function with weights
                beta = F.silu(patch)
                beta = beta.unsqueeze(1).repeat(1, self.out_channels, 1, 1, 1)
                beta_weights = self.beta_weights.unsqueeze(0).repeat(batch_size, 1, 1, 1, 1)
                beta = beta_weights * beta
                beta = beta.permute(1, 0, 2, 3, 4)
                
                # Evaluate the B-spline basis functions
                basis_values = self.b_spline_basis(patch)
                
                # Extend all the shapes to be compatible
                coeffs = self.coeffs.unsqueeze(2).repeat(1, 1, batch_size, 1, 1, 1)
                basis_values = basis_values.unsqueeze(1).repeat(1, self.out_channels, 1, 1, 1, 1)
                
                # Compute the linear combination of the basis functions
                splines = torch.sum(coeffs*basis_values, dim=0) # [batch_size, out_channels, in_channels, kernel_size[0], kernel_size[1]]
                spline_weights = self.spline_weights.unsqueeze(1).repeat(1, batch_size, 1, 1, 1)
                splines = spline_weights * splines
                
                # Find sum of the splines and beta values
                activations = splines + beta
                
                # Finally, compute the output part of the convolution
                result = torch.sum(activations, dim=(2, 3, 4)) # [batch_size, out_channels, in_channels, kernel_size[0], kernel_size[1]]
                result = result.permute(1, 0)
                output[:, :, i, j] = result

        return output
\end{lstlisting}