\chapter*{Висновки}
\markboth{Висновки}{Висновки}
\addcontentsline{toc}{chapter}{Висновки}

\hspace{\parindent} В цій роботі були розглянуті фундаментальні та формалізовані
принципи роботи нейронних мереж. Ми конкретизували основну проблематику 
машинного навчання та, зокрема, яка роль нейронних мереж у вирішенні цих
проблем. Ми навели визначення та теореми, що показують дві парадигми 
побудови архітектур нейронних мереж: мультишарові перцептрони (MLP) та 
мережі Колмогорова-Арнольда (KAN). Для обох парадигм ми навели теореми,
що доводять універсальність цих архітектур для апроксимації довільних 
функцій на гіперкубі $[0, 1]^m$. Ми також розглянули проблематику
вибору кількості шарів та нейронів у кожному з них, а також вибору
функції активації. Ми навели приклади використання нейронних мереж для
розв'язання задач класифікації і показали на практиці, що теорема 
Цибенко дійсно працює для складної функції. 

Попереду ще багато роботи. Зокрема, KAN мережі мають великий потенціал, проте 
кількість досліджень у цій галузі дуже обмежена. Окрім цього, багато речей 
з вище описаних можна формалізувати та описати більш строго: наприклад,
чому додавання шарів у MLP архітектурі не порушує властивість 
універсальної апроксимації або чи зміна активації, скажімо, на 
ReLU не змінює універсальність мережі.
